{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect livenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## return true for real and false for fake\n",
    "\n",
    "import cv2\n",
    "import dlib\n",
    "import numpy as np\n",
    "\n",
    "# Load the face detector and facial landmark predictor\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "# Function to detect facial landmarks\n",
    "def get_landmarks(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    rects = detector(gray, 0)\n",
    "\n",
    "    if len(rects) == 0:\n",
    "        return None\n",
    "\n",
    "    shape = predictor(gray, rects[0])\n",
    "    landmarks = [(shape.part(i).x, shape.part(i).y) for i in range(68)]\n",
    "    return landmarks\n",
    "\n",
    "# Function to calculate eye aspect ratio (EAR)\n",
    "def eye_aspect_ratio(eye):\n",
    "    # Compute the euclidean distances between the two sets of vertical eye landmarks\n",
    "    A = np.linalg.norm(np.array(eye[1]) - np.array(eye[5]))\n",
    "    B = np.linalg.norm(np.array(eye[2]) - np.array(eye[4]))\n",
    "\n",
    "    # Compute the euclidean distance between the horizontal eye landmarks\n",
    "    C = np.linalg.norm(np.array(eye[0]) - np.array(eye[3]))\n",
    "\n",
    "    # Compute the eye aspect ratio\n",
    "    ear = (A + B) / (2 * C)\n",
    "    return ear\n",
    "\n",
    "# Function to draw a frame around the detected face\n",
    "def draw_frame(image, landmarks):\n",
    "    overlay = image.copy()\n",
    "    for point in landmarks:\n",
    "        cv2.circle(overlay, point, 2, (0, 255, 0), -1)\n",
    "\n",
    "    alpha = 0.5\n",
    "    cv2.addWeighted(overlay, alpha, image, 1 - alpha, 0, image)\n",
    "\n",
    "# Main loop for liveness detection\n",
    "def detect_liveness():\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    blink_counter = 0\n",
    "    is_blinking = False\n",
    "    liveness = False\n",
    "    start_time = cv2.getTickCount()\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        landmarks = get_landmarks(frame)\n",
    "        if landmarks is None:\n",
    "            continue\n",
    "\n",
    "        left_eye = landmarks[36:42]\n",
    "        right_eye = landmarks[42:48]\n",
    "\n",
    "        left_ear = eye_aspect_ratio(left_eye)\n",
    "        right_ear = eye_aspect_ratio(right_eye)\n",
    "\n",
    "        avg_ear = (left_ear + right_ear) / 2\n",
    "\n",
    "        if avg_ear < 0.2:\n",
    "            if not is_blinking:\n",
    "                blink_counter += 1\n",
    "                is_blinking = True\n",
    "        else:\n",
    "            is_blinking = False\n",
    "        \n",
    "        draw_frame(frame, landmarks)\n",
    "\n",
    "        if blink_counter >= 1 or cv2.getTickCount() - start_time >= 15 * cv2.getTickFrequency():\n",
    "            liveness = True\n",
    "            cv2.putText(frame, \"Liveness: Real\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "            cv2.imshow(\"Face Liveness Detection\", frame)\n",
    "            break\n",
    "        else:\n",
    "            liveness = False\n",
    "            cv2.putText(frame, \"Liveness: Fake\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "            cv2.imshow(\"Face Liveness Detection\", frame)\n",
    "\n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(\"Liveness detected :\" ,liveness)\n",
    "    return liveness\n",
    "\n",
    "# Run the liveness detection\n",
    "#liveness_status = detect_liveness()\n",
    "#print(\"Liveness Status:\", liveness_status)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Face Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liveness detected : True\n",
      "Person is not verified. Kindly register yourself\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import pymongo\n",
    "import face_recognition\n",
    "\n",
    "# Connect to MongoDB\n",
    "client = pymongo.MongoClient(\"mongodb://localhost:27017\")\n",
    "db = client[\"face_recognition\"]\n",
    "collection = db[\"face recognition\"]\n",
    "\n",
    "# Function to compare face encodings\n",
    "def compare_face_encodings(face_encodings, face_encodings_list):\n",
    "    results = face_recognition.compare_faces(face_encodings_list, face_encodings)\n",
    "    if True in results:\n",
    "        index = results.index(True)\n",
    "        person = collection.find_one({\"id\": index + 1})\n",
    "        return person[\"name\"]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Function to capture an image using a webcam\n",
    "def capture_image():\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to capture image\")\n",
    "        return None\n",
    "    cap.release()\n",
    "    return frame\n",
    "\n",
    "# Function to extract face encodings from an image\n",
    "def extract_face_encodings(image):\n",
    "    face_locations = face_recognition.face_locations(image)\n",
    "    face_encodings = face_recognition.face_encodings(image, face_locations)\n",
    "    return face_encodings\n",
    "\n",
    "# Main program\n",
    "def main():\n",
    "    if detect_liveness():\n",
    "        # Capture an image using the webcam\n",
    "        image = capture_image()\n",
    "\n",
    "        # Check if the image was captured successfully\n",
    "        if image is None:\n",
    "            return\n",
    "\n",
    "        # Extract face encodings from the captured image\n",
    "        face_encodings = extract_face_encodings(image)\n",
    "\n",
    "        # Check if any face encodings were extracted\n",
    "        if len(face_encodings) == 0:\n",
    "            print(\"No face detected in the captured image\")\n",
    "            return\n",
    "\n",
    "        # Get the face encodings list from the database\n",
    "        cursor = collection.find()\n",
    "        face_encodings_list = [person[\"encodings\"] for person in cursor]\n",
    "\n",
    "        # Compare the face encodings with the database\n",
    "        name = compare_face_encodings(face_encodings[0], face_encodings_list)\n",
    "\n",
    "        if name is not None:\n",
    "            print(\"Person is verified.\")\n",
    "            print(\"Name of person is : \", name)\n",
    "        else:\n",
    "            print(\"Person is not verified. Kindly register yourself\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
